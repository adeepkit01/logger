<ujdcodr> : Let us begin<ujdcodr> : So does everyone remember that creepy looking sigmoid function? We're going to talk more about it<ujdcodr> : 1/(1+e^-x)<ujdcodr> : and the cost function is rewritten as y*(- log ( hyp )) + (1-y) * (- log ( 1 - hyp ))<ujdcodr> : no doubts about this right?<ujdcodr> : when y=1 we get the first half, when y=0 we get the second half of the cost function<ujdcodr> : where hyp is the sigmoid function<ujdcodr> : http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[5].png<ujdcodr> : take a look at the image<ujdcodr> : it's just a plot when y=1 and hyp is substitued with the sigmoid function<ujdcodr> : *substituted<ujdcodr> : http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[6].png<ujdcodr> : and this is when y=0<ujdcodr> : has everyone seen the image?<Swastik> : yeah<ujdcodr> : because we are going to define a new cost function<ujdcodr> : a small modification<ujdcodr> : http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[7].png<ujdcodr> : http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[8].png<ujdcodr> : notice the region of the curve that has been thresholded<ujdcodr> : in the first image anything beyond 1 is directly set to 0<ujdcodr> : and in the second image anything before -1 is set to 0<ujdcodr> : and then we have a straight line<ujdcodr> : how is all this helpful?<ujdcodr> : it Gives the SVM a computational advantage and an easier optimization problem<ujdcodr> : since the values are more clear and discrete, the algorithm will have an easier time computing the "right" result<ujdcodr> : everyone clear?<ujdcodr> : with the original and modified cost functions?<Swastik> : yeah<dharma> : yes<VS> : Yes<ujdcodr> : questions? anyone? This has a lot of math internally, but we're not going to go there<ujdcodr> : just understand that this modification reduces computational complexity<ujdcodr> : So what i defined here is an SVM cost function<Swastik> : How is better than the original one?<ujdcodr> : it works like a threshold<Swastik> : okay<ujdcodr> : there won't be values confused between 0 and 1<ujdcodr> : you will get either 0 or whatever lies on the slope of that straight line<Swastik> : Got it<ujdcodr> : which is much better than dealing with constantly varying values ona logarithmic curve right?<Swastik> : yeah<ujdcodr> : Moving on<ujdcodr> : What exactly is an SVM?<ujdcodr> : for that i'll have to tell you what a support vector is<ujdcodr> : https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_1.png<ujdcodr> : take a look<ujdcodr> : Support Vectors are simply the co-ordinates of individual observation.<ujdcodr> : they are just your "yes" and "no" that are graphed<ujdcodr> : That line that divides the 2 clusters is the "Machine"<ujdcodr> : “Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems.<ujdcodr> : In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate<ujdcodr> : Then, we perform classification by finding the hyper-plane that differentiate the two classes very well<ujdcodr> : From a bird's eye view... that's all that's there to SVMs<ujdcodr> : A simple concept, but a lot of mathematical rigor<ujdcodr> : We're not done yet<ujdcodr> : I will give you a small idea of the inner workings<ujdcodr> : But first let's the the "hyper plane" idea right<ujdcodr> : https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_21.png<ujdcodr> : which of A,B,C is the best hyperplane?<Swastik> : B?<ujdcodr> : Correct.... bottom line of SVMs:“Select the hyper-plane which segregates the two classes better” <ujdcodr> : every one clear with that?<Swastik> : yes<Ravali> : yeah<Vikram_> : Yes<dharma> : yes<ujdcodr> : the next one is straight forward but has a small concept behind it<ujdcodr> : https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_3.png<ujdcodr> : what abt this one?<dharma> : all<Swastik> : C?<ujdcodr> : @dharma yes...but which is the best one?<ujdcodr> : remember what SVM looks for<dharma> : den c<VS> : B<ujdcodr> : an SVM is also known as a "Large Margin Classifier"<ujdcodr> : Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin<ujdcodr> : you want to choose the line that's the farthest from the nearest support vectors of both classes<ujdcodr> : Hence you're trying to maximize the margin<ujdcodr> : Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification.<ujdcodr> : So if you chose A, a new data entry that might actually belong to the red circles might get classified as a blue star<ujdcodr> : hence "miss classified"<ujdcodr> : https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_5.png<ujdcodr> : how about this one?<Swastik> : B?<ujdcodr> : anyone else?<Vikram_> : B<VS> : B<dharma> : B<ujdcodr> : So what was your logic?<ujdcodr> : everyone.. just type a one line explanation <ujdcodr> : keep it simple<dharma> : high margin<Swastik> : ignoring the one misclassified value,the margin was high<ujdcodr> : "ignoring the one misclassified value", i didn't ask you to ignore it. Why did you?<ujdcodr> : XD<ujdcodr> : here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin<Vikram_> : If we choose A, probability of misclassification of red will high<ujdcodr> : SVM isn't worried about that initially<dharma> : okey<ujdcodr> : every algorithm has steps right? If-else conditions<ujdcodr> : maximizing the margin is not a priority at this point of time<ujdcodr> : we need to classify<ujdcodr> : and that's exactly what SVM does<ujdcodr> : the fact that there is a blue star over there in the distance means that any other example around that region has high chances of being a blue star<ujdcodr> : makes sense everyone?<ujdcodr> : 1. Classify<ujdcodr> : 2. Maximize margine<ujdcodr> : Therefore, the right hyper-plane is A